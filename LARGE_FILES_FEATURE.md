# Функциональность больших файлов

## Обзор

Система теперь поддерживает обработку больших файлов (более 20 МБ) с фоновой загрузкой в SQLite базу данных. Большие файлы не блокируют основной интерфейс и обрабатываются в фоновом режиме.

## Основные возможности

### 1. Автоматическое определение больших файлов
- Файлы размером более 20 МБ автоматически определяются как большие
- Большие файлы сохраняются на диск вместо загрузки в память
- Запускается фоновая обработка без блокировки интерфейса

### 2. Служебная база данных SQLite
- Хранение метаданных файлов
- Логирование процесса обработки
- Кэширование данных для быстрого доступа

### 3. Фоновая обработка
- Неблокирующая обработка больших файлов
- Пошаговая загрузка данных в SQLite
- Отслеживание прогресса обработки

### 4. Система логирования
- Детальные логи процесса обработки
- Отслеживание ошибок и предупреждений
- Возможность просмотра логов в реальном времени

## Архитектура

### Backend компоненты

#### SQLiteService (`/backend/app/services/sqlite_service.py`)
- Управление служебной базой данных SQLite
- Хранение метаданных файлов
- Логирование процесса обработки
- Кэширование данных

#### LargeFileProcessor (`/backend/app/services/large_file_processor.py`)
- Определение больших файлов (порог 20 МБ)
- Сохранение файлов на диск
- Фоновая обработка файлов
- Управление потоками обработки

### API эндпоинты

#### `/large-files`
- `GET /large-files` - получение списка всех больших файлов
- `GET /large-files/{file_id}` - информация о конкретном файле
- `GET /large-files/{file_id}/logs` - логи обработки файла
- `GET /large-files/{file_id}/data` - кэшированные данные файла
- `GET /large-files/status/all` - статус всех обрабатываемых файлов

### Frontend компоненты

#### LargeFileManager (`/frontend/src/components/LargeFileManager.tsx`)
- Отображение списка больших файлов
- Просмотр статуса обработки
- Просмотр логов в реальном времени
- Управление файлами

#### Обновленный DataPreview
- Специальное отображение для больших файлов
- Информация о статусе обработки
- Ссылки на детальную информацию

## Использование

### 1. Загрузка большого файла
1. Выберите файл размером более 20 МБ
2. Система автоматически определит его как большой
3. Файл будет сохранен на диск
4. Запустится фоновая обработка

### 2. Отслеживание обработки
1. Перейдите в раздел "Управление большими файлами"
2. Просматривайте статус обработки в реальном времени
3. Просматривайте логи для детальной информации

### 3. Просмотр результатов
1. После завершения обработки данные доступны в SQLite
2. Возможность экспорта обработанных данных
3. Просмотр статистики обработки

## Технические детали

### Пороги и ограничения
- **Порог больших файлов**: 20 МБ
- **Размер чанка обработки**: 1000 строк
- **Максимальный размер логов**: 100 записей на файл

### Поддерживаемые форматы
- CSV файлы
- JSON файлы  
- XML файлы

### Производительность
- Неблокирующая обработка
- Пошаговая загрузка данных
- Оптимизированное использование памяти
- Автоматическая очистка временных файлов

## Мониторинг и отладка

### Логи обработки
- Уровни: INFO, WARNING, ERROR
- Временные метки
- Детальные сообщения об ошибках

### Статусы файлов
- `uploaded` - файл загружен
- `processing` - обрабатывается
- `completed` - обработка завершена
- `error` - ошибка обработки

### Метрики
- Количество обработанных строк
- Время начала и завершения обработки
- Размер файла и тип
- Количество ошибок

## Безопасность

- Изоляция больших файлов в отдельной директории
- Контроль доступа к служебной базе данных
- Валидация типов файлов
- Ограничение размера загружаемых файлов

## Масштабирование

- Поддержка множественных потоков обработки
- Асинхронная обработка файлов
- Возможность горизонтального масштабирования
- Оптимизация для больших объемов данных

## Просмотр данных в SQLite

### Структура базы данных

Для каждого загруженного большого файла создается отдельная таблица в служебной базе данных SQLite:

- **Путь к базе**: `/app/data/service.db` (в контейнере backend)
- **Формат имени таблицы**: `file_data_{file_id}`
- **Структура таблицы**: колонки соответствуют заголовкам CSV файла + `row_index`

### Как посмотреть данные

#### 1. Через веб-интерфейс (рекомендуется)

Откройте раздел "Управление большими файлами" в веб-интерфейсе - там отображается статус обработки и есть кнопка "Просмотр данных" для каждого файла.

#### 2. Через API эндпоинты

```bash
# Получить список всех файлов
curl http://localhost:8000/large-files

# Получить данные из таблицы файла (первые 1000 строк)
curl http://localhost:8000/large-files/{file_id}/table-data

# Получить данные с пагинацией
curl "http://localhost:8000/large-files/{file_id}/table-data?limit=100&offset=0"
```

### Способы просмотра данных

#### 1. Через API эндпоинты

```bash
# Получить список всех файлов
curl http://localhost:8000/large-files

# Получить данные из таблицы файла (первые 1000 строк)
curl http://localhost:8000/large-files/{file_id}/table-data

# Получить данные с пагинацией
curl "http://localhost:8000/large-files/{file_id}/table-data?limit=100&offset=0"
```

#### 2. Прямое подключение к SQLite

```bash
# Подключиться к контейнеру backend
docker exec -it data-orchestrator-backend bash

# Открыть SQLite базу
sqlite3 /app/data/service.db

# Просмотреть все таблицы
.tables

# Просмотреть структуру таблицы файла
.schema file_data_1

# Выбрать данные из таблицы файла
SELECT * FROM file_data_1 LIMIT 10;

# Подсчитать количество строк
SELECT COUNT(*) FROM file_data_1;

# Просмотреть уникальные значения в колонке
SELECT DISTINCT column_name FROM file_data_1;
```

#### 3. Через веб-интерфейс SQLite

```bash
# Установить sqlite-web (если нужно)
pip install sqlite-web

# Запустить веб-интерфейс
sqlite_web /app/data/service.db --host 0.0.0.0 --port 8080
```

### Примеры SQL запросов

```sql
-- Просмотр метаданных файлов
SELECT * FROM file_metadata;

-- Просмотр логов обработки
SELECT * FROM processing_logs WHERE file_id = 1;

-- Просмотр данных файла с фильтрацией
SELECT * FROM file_data_1 WHERE column_name LIKE '%search_term%';

-- Агрегация данных
SELECT column_name, COUNT(*) as count 
FROM file_data_1 
GROUP BY column_name 
ORDER BY count DESC;

-- Поиск по нескольким колонкам
SELECT * FROM file_data_1 
WHERE column1 = 'value1' AND column2 > 100;
```

### Экспорт данных

```sql
-- Экспорт в CSV
.mode csv
.output export_file.csv
SELECT * FROM file_data_1;
.output stdout

-- Экспорт в JSON
.mode json
.output export_file.json
SELECT * FROM file_data_1;
.output stdout
```
